{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d467745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this change to allow code DensePhrases to run from a jupyter notebook\n",
    "# L227 in DPhrases/options.py:\n",
    "#     opt, unknown = self.parser.parse_known_args()  # opt = self.parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c947053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=../\n",
      "env: DATA_DIR=../densephrases-data\n",
      "env: SAVE_DIR=../outputs\n",
      "env: CACHE_DIR=../cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "\n",
    "%env BASE_DIR=../\n",
    "%env DATA_DIR=../densephrases-data\n",
    "%env SAVE_DIR=../outputs\n",
    "%env CACHE_DIR=../cache\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from integrate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4964e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS: Set inference parameters\n",
    "\n",
    "params = {\n",
    "    \"top_k\": 5,\n",
    "    \"use_large_index\": True,\n",
    "    \"strip_qmark\": False,\n",
    "    \"strip_qword1\": True,\n",
    "    \"strip_qword2\": False,\n",
    "    \"strip_qword_mode\": \"all\",  # first / all\n",
    "    \"prepend_hop_phrase\": False,\n",
    "    \"retrieval_unit\": \"phrase\",  # First hop only: phrase / sentence / paragraph\n",
    "    \"single_hop\": True,\n",
    "    \"mult_path_scores\": True\n",
    "}\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd82e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DensePhrases module...\n",
      "This could take up to 15 mins depending on the file reading speed of HDD/SSD\n",
      "Loading DensePhrases Completed!\n"
     ]
    }
   ],
   "source": [
    "# Load the DensePhrases module\n",
    "print(\"Loading DensePhrases module...\")\n",
    "model = load_densephrase_module(load_dir=load_dir, \n",
    "                                dump_dir=dump_dir, \n",
    "                                index_name=idx_name.replace('_small', ('' if params[\"use_large_index\"] \\\n",
    "                                                                       else '_small')), \n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a241af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /gypsum/scratch1/dagarwal/multihop_dense_retrieval/data/hotpot/hotpot_qas_val.json\n",
      "Found 5918 bridge questions in 7405 total questions.\n"
     ]
    }
   ],
   "source": [
    "# Read questions and answers from data_path\n",
    "queries = read_queries(data_path)\n",
    "questions = get_key(queries, 'question')\n",
    "answers = get_key(queries, 'answer')\n",
    "answers = [answer[0] for answer in answers]  # flattening\n",
    "\n",
    "# Setup function arguments based on the parameters\n",
    "method = 'pre' if params[\"prepend_hop_phrase\"] else 'post'\n",
    "top_k = params[\"top_k\"]\n",
    "ret_unit1 = params[\"retrieval_unit\"]\n",
    "strip_ques1 = params[\"strip_qmark\"]\n",
    "strip_prompt1 = params[\"strip_qword1\"]\n",
    "strip_ques2 = params[\"strip_qmark\"]\n",
    "strip_prompt2 = params[\"strip_qword2\"]\n",
    "strip_prompt_mode = params[\"strip_qword_mode\"]\n",
    "single_hop = params[\"single_hop\"]\n",
    "mult_path_scores = params[\"mult_path_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "698b552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched multi-hop inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 60/60 [12:17<00:00, 12.29s/it]\n"
     ]
    }
   ],
   "source": [
    "DEBUG=False\n",
    "if DEBUG:\n",
    "    ques = questions[:20]\n",
    "else:\n",
    "    ques = questions\n",
    "    \n",
    "# Run batched inference\n",
    "results = []\n",
    "print(\"Running batched multi-hop inference...\")\n",
    "for i in tqdm(range(0, len(ques), batch_size)):\n",
    "    batch_results = run_batch_inference(model, ques[i:i + batch_size], strip_ques1, strip_prompt1,\n",
    "                                        strip_ques2, strip_prompt2, ret_unit1, ret_unit2, ques_terms, method,\n",
    "                                        strip_prompt_mode, answers=answers[i:i + batch_size], write=False, \n",
    "                                        top_k=top_k, silent=True, single_hop=single_hop, mult_scores=mult_path_scores)\n",
    "    results += batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f8fe8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved at singlehop_1648136621.json\n",
      "Run metadata saved at singlehop_1648136621_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Write predictions to disk\n",
    "run_id = __import__(\"calendar\").timegm(__import__(\"time\").gmtime())\n",
    "if not single_hop:\n",
    "    out_file = f'predictions_{run_id}.json'\n",
    "else:\n",
    "    out_file = f'singlehop_{run_id}.json'\n",
    "meta_out_file = out_file.replace('.json', '_meta.json')\n",
    "with open(out_file, 'w') as fp:\n",
    "    json.dump(results, fp, indent=4)\n",
    "with open(meta_out_file, 'w') as fp:\n",
    "    json.dump(params, fp, indent=4)\n",
    "print(f\"Predictions saved at {out_file}\")\n",
    "print(f\"Run metadata saved at {meta_out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28aa64ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\r\n",
      "__pycache__\r\n",
      "evaluate.py\r\n",
      "evaluate_notebook.ipynb\r\n",
      "integrate.py\r\n",
      "integrate_notebook.ipynb\r\n",
      "predictions_1648115963.json\r\n",
      "predictions_1648115963_eval.json\r\n",
      "predictions_1648115963_meta.json\r\n",
      "predictions_1648127590.json\r\n",
      "predictions_1648127590_eval.json\r\n",
      "predictions_1648127590_meta.json\r\n",
      "predictions_1648132986.json\r\n",
      "predictions_1648132986_eval.json\r\n",
      "predictions_1648132986_meta.json\r\n",
      "singlehop_1648033166.json\r\n",
      "singlehop_1648033166_eval.json\r\n",
      "singlehop_1648033166_meta.json\r\n",
      "singlehop_1648036353.json\r\n",
      "singlehop_1648036353_eval.json\r\n",
      "singlehop_1648036353_meta.json\r\n",
      "singlehop_1648036353_predictions_1648115963_eval.json\r\n",
      "singlehop_1648036353_predictions_1648127590_eval.json\r\n",
      "singlehop_1648041365.json\r\n",
      "singlehop_1648041365_eval.json\r\n",
      "singlehop_1648041365_meta.json\r\n",
      "singlehop_1648136621.json\r\n",
      "singlehop_1648136621_meta.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0176bc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?',\n",
       " 'gold_answer': 'Chief of Protocol',\n",
       " 'predicted_answers': ['Shirley Temple',\n",
       "  'Joan Caulfield as Corliss Archer',\n",
       "  'Joan Caulfield',\n",
       "  'Shirley Temple in the role of Corliss Archer.',\n",
       "  'book editor'],\n",
       " 'predicted_titles': [['Kiss and Tell (1945 film)'],\n",
       "  ['Kiss and Tell (play)'],\n",
       "  ['Kiss and Tell (play)'],\n",
       "  ['Kiss and Tell (play)'],\n",
       "  ['The Kiss (2003 film)']]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d396c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
