{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d467745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this change to allow code DensePhrases to run from a jupyter notebook\n",
    "# L227 in DPhrases/options.py:\n",
    "#     opt, unknown = self.parser.parse_known_args()  # opt = self.parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c947053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=../\n",
      "env: DATA_DIR=../densephrases-data\n",
      "env: SAVE_DIR=../outputs\n",
      "env: CACHE_DIR=../cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "\n",
    "%env BASE_DIR=../\n",
    "%env DATA_DIR=../densephrases-data\n",
    "%env SAVE_DIR=../outputs\n",
    "%env CACHE_DIR=../cache\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from integrate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4964e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS: Set inference parameters\n",
    "\n",
    "params = {\n",
    "    \"top_k\": 5,\n",
    "    \"use_large_index\": True,\n",
    "    \"strip_qmark\": False,\n",
    "    \"strip_qword1\": True,\n",
    "    \"strip_qword2\": False,\n",
    "    \"strip_qword_mode\": \"all\",  # first / all\n",
    "    \"prepend_hop_phrase\": False,\n",
    "    \"retrieval_unit\": \"phrase\",  # phrase / sentence / paragraph\n",
    "    \"single_hop\": False\n",
    "}\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd82e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DensePhrases module...\n",
      "This could take up to 15 mins depending on the file reading speed of HDD/SSD\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b0bd0c1bba4b2a824a119ddd4c3857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DensePhrases Completed!\n"
     ]
    }
   ],
   "source": [
    "# Load the DensePhrases module\n",
    "print(\"Loading DensePhrases module...\")\n",
    "model = load_densephrase_module(load_dir=load_dir, \n",
    "                                dump_dir=dump_dir, \n",
    "                                index_name=idx_name.replace('_small', ('' if params[\"use_large_index\"] \\\n",
    "                                                                       else '_small')), \n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a241af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /gypsum/scratch1/dagarwal/multihop_dense_retrieval/data/hotpot/hotpot_qas_val.json\n",
      "Found 5918 bridge questions in 7405 total questions.\n"
     ]
    }
   ],
   "source": [
    "# Read questions and answers from data_path\n",
    "queries = read_queries(data_path)\n",
    "questions = get_key(queries, 'question')\n",
    "answers = get_key(queries, 'answer')\n",
    "answers = [answer[0] for answer in answers]  # flattening\n",
    "\n",
    "# Setup function arguments based on the parameters\n",
    "method = 'pre' if params[\"prepend_hop_phrase\"] else 'post'\n",
    "top_k = params[\"top_k\"]\n",
    "ret_unit1 = params[\"retrieval_unit\"]\n",
    "strip_ques1 = params[\"strip_qmark\"]\n",
    "strip_prompt1 = params[\"strip_qword1\"]\n",
    "strip_ques2 = params[\"strip_qmark\"]\n",
    "strip_prompt2 = params[\"strip_qword2\"]\n",
    "strip_prompt_mode = params[\"strip_qword_mode\"]\n",
    "single_hop = params[\"single_hop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "698b552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched multi-hop inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                   | 0/60 [00:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 10.76 GiB total capacity; 3.72 GiB already allocated; 225.44 MiB free; 4.99 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/tmp/login/ipykernel_3576549/2530066397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                         \u001b[0mstrip_ques2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_prompt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_unit1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_unit2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                         \u001b[0mstrip_prompt_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                         top_k=top_k, silent=True, single_hop=single_hop)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dagarwal_umass_edu/DAQuA-Difficulty-Aware-Question-Answering/DPhrases/no_ftune_hotpot_eval/integrate.py\u001b[0m in \u001b[0;36mrun_batch_inference\u001b[0;34m(model, query_list, strip_ques1, strip_prompt1, strip_ques2, strip_prompt2, ret_unit1, ret_unit2, ques_terms, method, strip_prompt_mode, answers, write, top_k, silent, single_hop, debug)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: Pruning resulted in <top-K results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mhop_retunit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Get score and phrase list from second retrieval for evidence chain extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dagarwal_umass_edu/DAQuA-Difficulty-Aware-Question-Answering/DPhrases/no_ftune_hotpot_eval/integrate.py\u001b[0m in \u001b[0;36mrecurse\u001b[0;34m(i, silent)\u001b[0m\n\u001b[1;32m    306\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetching top-k={top_k*(2**i)} results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 hop_retunit, hop_metadata = model.search(upd_flat_second_hop_qlist, retrieval_unit=ret_unit2, top_k=top_k*(2**i),\n\u001b[0;32m--> 308\u001b[0;31m                                                  return_meta=True)\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0mhop_retunit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_to_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhop_retunit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mhop_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_to_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhop_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dagarwal_umass_edu/DensePhrases/densephrases/model.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, retrieval_unit, top_k, truecase, return_meta)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_top_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_answer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mreturn_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_strat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_strats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mretrieval_unit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mreturn_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretrieval_unit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sentence'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dagarwal_umass_edu/DensePhrases/densephrases/index.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, q_texts, nprobe, top_k, aggregate, return_idxs, max_answer_length, agg_strat, return_sent)\u001b[0m\n\u001b[1;32m    467\u001b[0m         outs = self.search_phrase(\n\u001b[1;32m    468\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_doc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_answer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_answer_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         )\n\u001b[1;32m    471\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top-{top_k} phrase search: {time()-start_time:.3f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dagarwal_umass_edu/DensePhrases/densephrases/index.py\u001b[0m in \u001b[0;36msearch_phrase\u001b[0;34m(self, query, start_doc_idxs, start_idxs, orig_start_idxs, end_doc_idxs, end_idxs, orig_end_idxs, start_scores, end_scores, top_k, max_answer_length, return_idxs, return_sent)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for OPQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0mquery_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mnew_start_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 10.76 GiB total capacity; 3.72 GiB already allocated; 225.44 MiB free; 4.99 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "DEBUG=False\n",
    "if DEBUG:\n",
    "    ques = questions[:20]\n",
    "else:\n",
    "    ques = questions\n",
    "    \n",
    "# Run batched inference\n",
    "results = []\n",
    "print(\"Running batched multi-hop inference...\")\n",
    "for i in tqdm(range(0, len(ques), batch_size)):\n",
    "    batch_results = run_batch_inference(model, ques[i:i + batch_size], strip_ques1, strip_prompt1,\n",
    "                                        strip_ques2, strip_prompt2, ret_unit1, ret_unit2, ques_terms, method,\n",
    "                                        strip_prompt_mode, answers=answers[i:i + batch_size], write=False, \n",
    "                                        top_k=top_k, silent=True, single_hop=single_hop)\n",
    "    results += batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fe8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to disk\n",
    "run_id = __import__(\"calendar\").timegm(__import__(\"time\").gmtime())\n",
    "if not single_hop:\n",
    "    out_file = f'predictions_{run_id}.json'\n",
    "else:\n",
    "    out_file = f'singlehop_{run_id}.json'\n",
    "meta_out_file = out_file.replace('.json', '_meta.json')\n",
    "with open(out_file, 'w') as fp:\n",
    "    json.dump(results, fp, indent=4)\n",
    "with open(meta_out_file, 'w') as fp:\n",
    "    json.dump(params, fp, indent=4)\n",
    "print(f\"Predictions saved at {out_file}\")\n",
    "print(f\"Run metadata saved at {meta_out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dbeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
